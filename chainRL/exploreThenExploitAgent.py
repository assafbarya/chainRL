
from agentInterface import AgentInterface
import numpy as np

class ExploreThenExploitAgent( AgentInterface ):
    """first explores the state space, then calculates the policy"""


    def description( self ):
        return 'Explore-Exploit Agent'

    def __init__(   self, 
                    numStates, 
                    numActions, 
                    discountFactor         = 0.95, 
                    explorationStageLength = 10000, 
                    calcSteps              = 1000,
                    **kwargs ):

        self.discountFactor          = discountFactor
        self.numActions              = numActions
        self.numStates               = numStates
        self.R                       = np.zeros( ( numStates, numActions, numStates ) )
        self.T                       = np.zeros( ( numStates, numActions, numStates ) )
        self.explorationStageLength  = explorationStageLength 
        self.stepNum                 = 0
        self.calcSteps               = calcSteps

    def getAction( self ):
        if self.stepNum <= self.explorationStageLength:
            self.action = np.random.randint( 0, self.numActions )
        else:
            self.action = self.Q[ self.state ].argmax()

        self.stepNum += 1
        return self.action 

    def update( self, nextState, reward ):
        if self.stepNum < self.explorationStageLength:
            self.T[ self.state, self.action, nextState ] += 1
            self.R[ self.state, self.action, nextState ] += reward

        elif self.stepNum == self.explorationStageLength:
            ## Calculate reward matrix
            self.R /= self.T
            self.R[ np.isnan( self.R ) ] = 0
            
            ## Calculate transition prob matrix
            P = self.T
            for s in range( self.numStates ):
                for a in range( self.numActions ):
                    sum = self.T[ s, a ].sum()
                    for sp in range( self.numStates ):
                        P[ s, a, sp ] /= sum

            ## Calculate Q
            self.Q = np.zeros( ( self.numStates, self.numActions ) )
            for _ in range( self.calcSteps ):
                newQ = np.zeros( ( self.numStates, self.numActions ) )
                for s in range( self.numStates ):
                    for a in range( self.numActions ):
                        for sp in range( self.numStates ):
                            newQ[ s, a ] += P[ s, a, sp ] * ( self.R[ s, a, sp ] + self.discountFactor * self.Q[ sp ].max() )
                self.Q = newQ

        self.setState( nextState )

    def setState( self, state ):
        self.state = state

    def printInternalState( self ):
        print ( self.Q )
